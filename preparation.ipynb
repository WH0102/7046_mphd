{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (6.29.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: matplotlib in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: openpyxl in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: pandas in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: pip in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (24.0)\n",
      "Requirement already satisfied: plotly_express in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: polars in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (0.20.15)\n",
      "Requirement already satisfied: PyCap in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: seaborn in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (0.13.2)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-69.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: tabulate in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pyspark[pandas_on_spark] in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (8.22.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (24.0)\n",
      "Requirement already satisfied: psutil in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipykernel) (5.14.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: et-xmlfile in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: plotly>=4.1.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from plotly_express) (5.20.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from plotly_express) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.18 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from plotly_express) (1.12.0)\n",
      "Requirement already satisfied: patsy>=0.5 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from plotly_express) (0.5.6)\n",
      "Requirement already satisfied: requests<3.0,>=2.20 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from PyCap) (2.31.0)\n",
      "Requirement already satisfied: semantic-version<3.0.0,>=2.8.5 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from PyCap) (2.10.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (0.10.9.7)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from pyspark[pandas_on_spark]) (15.0.1)\n",
      "Requirement already satisfied: decorator in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: colorama in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (306)\n",
      "Requirement already satisfied: six in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from patsy>=0.5->plotly_express) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from plotly>=4.1.0->plotly_express) (8.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from requests<3.0,>=2.20->PyCap) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from requests<3.0,>=2.20->PyCap) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from requests<3.0,>=2.20->PyCap) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from requests<3.0,>=2.20->PyCap) (2024.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\downloads\\documents\\vscode\\.venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Using cached setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
      "Installing collected packages: setuptools\n",
      "Successfully installed setuptools-69.2.0\n",
      "Package            VersionNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "------------------ -----------\n",
      "asttokens          2.4.1\n",
      "certifi            2024.2.2\n",
      "charset-normalizer 3.3.2\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "contourpy          1.2.0\n",
      "cycler             0.12.1\n",
      "debugpy            1.8.1\n",
      "decorator          5.1.1\n",
      "et-xmlfile         1.1.0\n",
      "executing          2.0.1\n",
      "fonttools          4.50.0\n",
      "idna               3.6\n",
      "ipykernel          6.29.3\n",
      "ipython            8.22.2\n",
      "jedi               0.19.1\n",
      "jupyter_client     8.6.1\n",
      "jupyter_core       5.7.2\n",
      "kiwisolver         1.4.5\n",
      "matplotlib         3.8.3\n",
      "matplotlib-inline  0.1.6\n",
      "nest-asyncio       1.6.0\n",
      "numpy              1.26.4\n",
      "openpyxl           3.1.2\n",
      "packaging          24.0\n",
      "pandas             2.2.1\n",
      "parso              0.8.3\n",
      "patsy              0.5.6\n",
      "pillow             10.2.0\n",
      "pip                24.0\n",
      "platformdirs       4.2.0\n",
      "plotly             5.20.0\n",
      "plotly-express     0.4.1\n",
      "polars             0.20.15\n",
      "prompt-toolkit     3.0.43\n",
      "psutil             5.9.8\n",
      "pure-eval          0.2.2\n",
      "py4j               0.10.9.7\n",
      "pyarrow            15.0.1\n",
      "pycap              2.6.0\n",
      "Pygments           2.17.2\n",
      "pyparsing          3.1.2\n",
      "pyspark            3.5.1\n",
      "python-dateutil    2.9.0.post0\n",
      "python-dotenv      1.0.1\n",
      "pytz               2024.1\n",
      "pywin32            306\n",
      "pyzmq              25.1.2\n",
      "requests           2.31.0\n",
      "scipy              1.12.0\n",
      "seaborn            0.13.2\n",
      "semantic-version   2.10.0\n",
      "setuptools         69.2.0\n",
      "six                1.16.0\n",
      "stack-data         0.6.3\n",
      "statsmodels        0.14.1\n",
      "tabulate           0.9.0\n",
      "tenacity           8.2.3\n",
      "tornado            6.4\n",
      "traitlets          5.14.2\n",
      "tzdata             2024.1\n",
      "urllib3            2.2.1\n",
      "wcwidth            0.2.13\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade ipykernel matplotlib numpy openpyxl pandas pip plotly_express polars PyCap python-dotenv pyspark[pandas_on_spark] seaborn setuptools tabulate\n",
    "# %pip install pandas==1.5.3\n",
    "# %pip install distutils\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "str_file = \"fix_Peka40STR2023.txt\"\n",
    "address_cols = [\"NoKPKIR\", \"Alamat\", \"Poskod\", \"Bandar\", \"Negeri\"]\n",
    "\n",
    "df = pl.scan_csv(str_file, separator=\"|\", infer_schema_length=10000000)\n",
    "df1 = df.select(address_cols).head(20).collect()\n",
    "\n",
    "# for item in address_cols[2:]:\n",
    "#     # print(item)\n",
    "#     df1.with_columns(pl.col(\"Alamat\").str.replace_all(pl.col(item).cast(pl.String), \"\"))\n",
    "\n",
    "print(df1.to_pandas().to_markdown(tablefmt = \"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scan_csv(\"fix_Peka40STR2023.txt\", separator=\"|\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------------------------------------------------------------+\n",
      "|NoKPKIR     |address                                                                                                  |\n",
      "+------------+---------------------------------------------------------------------------------------------------------+\n",
      "|000101010049|LOT 8091, KAMPUNG SENGKANG BATU 18 TANGKAK 84800 BUKIT GAMBIR JOHOR                                      |\n",
      "|000101010057|NO 21 A PARIT AMAL LORONG HAJI SUHAIMI JALAN SRI MUDA 84000 MUAR JOHOR                                   |\n",
      "|000101010073|NO34 TINGKAT 2 BLOCK B TAMAN DESA RAKYAT PERDANA 81700 PASIR GUDANG JOHOR                                |\n",
      "|000101010081|NO 13, JALAN TERKUKUR, LARKIN JAYA 80350 JOHOR BAHRU JOHOR                                               |\n",
      "|000101010102|BLOK A4, 02-10 JALAN SEROJA INDAH 2 TAMAN SEROJA 81200 JOHOR BAHRU JOHOR                                 |\n",
      "|000101010110|NO 31 JALAN SIERRA PERDANA 1/5 TAMAN SIERRA PERDANA 81750 MASAI JOHOR                                    |\n",
      "|000101010129|20/3 LORONG A4 JALAN BUNGA RAYA KAMPUNG MELAYU 86000 KLUANG JOHOR                                        |\n",
      "|000101010137|NO 37-C KAMPUNG PARIT BUGIS BATU PAHAT 83200 SENGGARANG JOHOR                                            |\n",
      "|000101010145|6 JALAN RAMUNIA 4 DESA RAMUNIA 81620 PENGERANG JOHOR                                                     |\n",
      "|000101010153|NO6 TAMAN PASIR RAYA BATU6 1/4 84300 BUKIT PASIR JOHOR                                                   |\n",
      "|000101010196|NO 36, LORONG 10, JALAN 8/19, PERJIRANAN 8, TAMAN AIR BIRU 81700 PASIR GUDANG JOHOR                      |\n",
      "|000101010217|NO 6 JALAN PADI MAHSURI 6 BANDAR BARU UDA 81200 JOHOR BAHRU JOHOR                                        |\n",
      "|000101010225|POS 123, TL 191, KG SG GERSIK DARAT BATU PAHAT 83600 SEMERAH JOHOR                                       |\n",
      "|000101010241|NO 3 JALAN DELIMA 1 TAMAN DELIMA 1/2 BAHRU 81100 JOHOR BAHRU JOHOR                                       |\n",
      "|000101010268|NO 2 JALAN MATAHARI 4 TAMAN MATAHARI 1 86000 KLUANG JOHOR                                                |\n",
      "|000101010356|15 JALAN LEMBAH 21 BANDAR SERI ALAM 81750 MASAI JOHOR                                                    |\n",
      "|000101010364|NO. 146, JALAN SONGKET 2, TAMAN ANJARIA 81000 KULAI JOHOR                                                |\n",
      "|000101010399|NO 84, LORONG LASSIM, JALAN HAJI ABDULLAH, . 84000 MUAR JOHOR                                            |\n",
      "|000101010401|NO 9 JALAN PULAI JAYA 2/7 BANDAR BARU PULAI JAYA SKUDAI BAHRU 81300 JOHOR BAHRU JOHOR                    |\n",
      "|000101010436|J80, KAMPUNG JALAN RAJA, MUAR 84300 BUKIT PASIR JOHOR                                                    |\n",
      "|000101010444|NO47-B PERINGKAT 2 JALAN ROS KAMPUNG CAHAYA BARU 81760 MASAI JOHOR                                       |\n",
      "|000101010452|NO 6, LORONG HAJI RASHID 2, JALAN LAMA 83700 YONG PENG JOHOR                                             |\n",
      "|000101010479|NO 20 JALAN SRI MUDA PARIT AMAL DARAT 84000 MUAR JOHOR                                                   |\n",
      "|000101010524|NO 22 JALAN HARMONIUM 22/12 TAMAN DESA TEBRAU BAHRU 81100 JOHOR BAHRU JOHOR                              |\n",
      "|000101010567|1105 BLOK 21 FELDA AIR TAWAR 2 81900 KOTA TINGGI JOHOR                                                   |\n",
      "|000101010583|NO 29 PARIT BAKAR TENGAH CHINESE 84000 MUAR JOHOR                                                        |\n",
      "|000101010647|KM 21, KAMPUNG PARIT ZING, SUNGAI MATI, TANGKAK 84410 LEDANG JOHOR                                       |\n",
      "|000101010655|NO 8 JALAN MEDOI KAMPUNG JAWA 85000 SEGAMAT JOHOR                                                        |\n",
      "|000101010727|NO 10 JALAN DENAI 15 TAMAN BUKIT JAYA 81800 ULU TIRAM JOHOR                                              |\n",
      "|000101010743|NO2JALAN PERMATA TAMAN NORA 81800 ULU TIRAM JOHOR                                                        |\n",
      "|000101010778|2, JALAN PERMAS 3/3 BANDAR BARU PERMAS JAYA 81750 MASAI JOHOR                                            |\n",
      "|000101010794|NO 30 JLN LEMBING 3 TMN PUTERI WANGSA 81800 ULU TIRAM JOHOR                                              |\n",
      "|000101010823|NO 1 JLN PT HJ WAHAB KAYU ARA PASONG 82010 PONTIAN JOHOR                                                 |\n",
      "|000101010874|DH1188, JALAN TERATAI 10 TAMAN BUNGA TERATAI 81500 PEKAN NENAS JOHOR                                     |\n",
      "|000101010890|NO 371 JALAN TREH 7 TAMAN SRI TREH 1 84000 MUAR JOHOR                                                    |\n",
      "|000101010911|20 JALAN UTAMA 25 TAMAN MUTIARA RINI SKUDAI BAHRU 81300 JOHOR BAHRU JOHOR                                |\n",
      "|000101010946|ST 89, SUNGAI TERAP, MUAR 84300 BUKIT PASIR JOHOR                                                        |\n",
      "|000101011033|NO 58, KAMPUNG RAHMAT 86000 KLUANG JOHOR                                                                 |\n",
      "|000101011068|NO 87 JALAN LEE KAY HOO PEKAN JABI 85000 SEGAMAT JOHOR                                                   |\n",
      "|000101011068|NO 87 JALAN LEE KAY HOO PEKAN JABI 85000 SEGAMAT JOHOR                                                   |\n",
      "|000101011084|12, JALAN PONDEROSA 1/10, TAMAN PONDEROSA 81100 JOHOR BAHRU JOHOR                                        |\n",
      "|000101011113|NO 16 JALAN SETIA 4/35 TAMAN SETIA INDAH 81100 JOHOR BAHRU JOHOR                                         |\n",
      "|000101011148|NO 54 JALAN 24 TAMAN SERAYA 56100 KUALA LUMPUR KUALA LUMPUR                                              |\n",
      "|000101011148|NO 54 JALAN 24 TAMAN SERAYA 56100 KUALA LUMPUR KUALA LUMPUR                                              |\n",
      "|000101011199|27-B JALAN JALAK, KAMPUNG SEPINANG 85000 SEGAMAT JOHOR                                                   |\n",
      "|000101011201|NO 432 JALAN PADI MAHSURI 9 TAMAN BARU 81400 SENAI JOHOR                                                 |\n",
      "|000101011252|NO.172-18 JALAN TENGKU BENDAHARA 84000 MUAR JOHOR                                                        |\n",
      "|000101011332|NO 7, JALAN BESTARI 32/3 TAMAN BESTARI INDAH 81800 ULU TIRAM JOHOR                                       |\n",
      "|000101011359|NO 109 KG BARU 81550 GELANG PATAH JOHOR                                                                  |\n",
      "|000101011375|NO 63 JALAN PULASAN 41 TAMAN KOTA MASAI 81700 PASIR GUDANG JOHOR                                         |\n",
      "|000101011391|NO 6 JLN NB2 6/4 TAMAN NUSA BESTARI 2 81300 JOHOR BAHRU JOHOR                                            |\n",
      "|000101011404|01-09 BLOK LM 11 JALAN ENAU 16 TAMAN DAYA 81100 JOHOR BAHRU JOHOR                                        |\n",
      "|000101011420|NO 195 JALAN SUTERA 6 TAMAN ANJARIA 81000 KULAI JOHOR                                                    |\n",
      "|000101011439|NO 104, JALAN 5 TAMAN BERTUAH SATU 86000 KLUANG JOHOR                                                    |\n",
      "|000101011447|18, JALAN MUTIARA UTAMA 8 TAMAN MUTIARA UTAMA SKUDAI 81300 JOHOR BAHRU JOHOR                             |\n",
      "|000101011455|NO1 JALAN BERKAT KAMPUNG MELAYU MAJIDI BAHRU 81100 JOHOR BAHRU JOHOR                                     |\n",
      "|000101011463|NO 10 JALAN RAJAWALI KG USAHA JAYA 85300 LABIS JOHOR                                                     |\n",
      "|000101011471|NO, 149 JALAN MAKMUR 4 TAMAN MAKMUR 81000 KULAI JOHOR                                                    |\n",
      "|000101011498|5, JALAN MOLEK 3/15 TAMAN MOLEK 81100 JOHOR BAHRU JOHOR                                                  |\n",
      "|000101011527|22/3 KAMPUNG PARIT KASEH, SENGGARA BATU PAHAT 83200 SENGGARANG JOHOR                                     |\n",
      "|000101011535|NO. 95, JALAN PARIT JABAR, BATU PAHAT 83200 SENGGARANG JOHOR                                             |\n",
      "|000101011586|NO 3 JALAN RAMBAI 4 TAMAN KOTA JAYA 81900 KOTA TINGGI JOHOR                                              |\n",
      "|000101011666|NO 25 JALAN SALAMUN KAWASAN AYER HITAM KG SETIA JAYA 86200 SIMPANG RENGAM JOHOR                          |\n",
      "|000101011682|LOT 1548 BATU 14 3/4 JALAN SULATI SUHOOD 81800 ULU TIRAM JOHOR                                           |\n",
      "|000101011690|NO916 TAMAN JAYA 73400 GEMAS NEGERI SEMBILAN                                                             |\n",
      "|000101011738|NO 59 JALAN BUNGA RAYA 2 TAMAN IRAMA 71600 KUALA KLAWANG NEGERI SEMBILAN                                 |\n",
      "|000101011770|137 BLOK 7 FELDA AIR TAWAR 3 81920 AYER TAWAR 2 JOHOR                                                    |\n",
      "|000101011797|NO 11, JALAN RAMBUTAN 1 TAMAN WANGI 83700 YONG PENG JOHOR                                                |\n",
      "|000101011834|NO 20, JALAN PERJIRANAN 10/16 BANDAR DATO ONN 81100 JOHOR BAHRU JOHOR                                    |\n",
      "|000101011842|15 JALAN PUTRA 16 TAMAN TAN SRI YAAKOB 8 15 JALAN PUTRA 16 TAMAN TAN SRI YAAKOB 8 81300 JOHOR BAHRU JOHOR|\n",
      "|000101011869|NO18B KAMPUNG SEPARAP MUKIM 4 83000 BATU PAHAT JOHOR                                                     |\n",
      "|000101011877|NO 58 KAMPUNG PARIT HAJI SIRAJ 86100 AYER HITAM JOHOR                                                    |\n",
      "|000101011893|NO32, JALAN INDAH 5 TAMAN BUKIT INDAH 86100 AYER HITAM JOHOR                                             |\n",
      "|000101011906|NO.70 JALAN PERWIRA 6 TAMAN JAYA 85400 CHAAH JOHOR                                                       |\n",
      "|000101011957|NO13 JALAN BINA 7 TAMAN, BATU PAHAT 86400 PARIT RAJA JOHOR                                               |\n",
      "|000101011965|43-A TL PTL 176-3 KG PT TENGAH SRI MERLONG 83100 RENGIT JOHOR                                            |\n",
      "|000101011973|SBK 227 KAMPUNG SUNGAI BALANG KECIL 83610 MUAR JOHOR                                                     |\n",
      "|000101012298|NO 41, KAMPUNG PARIT LAPIS KADIR 83210 SENGGARANG JOHOR                                                  |\n",
      "|000101012378|NO 14, JALAN DAMAI 3/1 TAMAN DAMAI 3 83000 BATU PAHAT JOHOR                                              |\n",
      "|000101012386|NO 4 JLN DAMAI 3 TAMAN DAMAI 83200 SENGGARANG JOHOR                                                      |\n",
      "|000101012394|JYP24/4 LORONG BUKIT CIKU PEKAN PARIT YAANI 83710 YONG PENG JOHOR                                        |\n",
      "|000101012466|02-22 BLOK 24 TAMAN CEMPAKA 81200 JOHOR BAHRU JOHOR                                                      |\n",
      "|000101020052|2485, JALAN PERMATA 19 TAMAN PERMATA 53300 KUALA LUMPUR KUALA LUMPUR                                     |\n",
      "|000101020079|KAMPUNG LAWER MUKIM PADANG MATSIRAT 07000 LANGKAWI KEDAH                                                 |\n",
      "|000101020095|NO 178 KAMPUNG HAJI SAAD JALAN IBRAHIM 06000 JITRA KEDAH                                                 |\n",
      "|000101020124|87 JALAN ANGGERIK 8/5 BANDAR AMANJAYA 08000 SUNGAI PETANI KEDAH                                          |\n",
      "|000101020159|NO 10-1068 FELDA GUAR NAPAI 06100 KODIANG KEDAH                                                          |\n",
      "|000101020167|LOT 27 RANCANGAN TERATUR HUTAN GELAM JALAN TITI HAJI IDRIS 06500 LANGGAR KEDAH                           |\n",
      "|000101020175|KAMPUNG LUBUK BATU BALING 09110 KUALA PEGANG KEDAH                                                       |\n",
      "|000101020183|KAMPUNG GENTING PALAS 06300 KUALA NERANG KEDAH                                                           |\n",
      "|000101020191|80B JALAN TAMAN LEMBAH BUJANG 7/2 TAMAN LEMBAH BUJANG 08100 BEDONG KEDAH                                 |\n",
      "|000101020204|265, JALAN BPJ 1/3 BANDAR PUTERI JAYA 08000 SUNGAI PETANI KEDAH                                          |\n",
      "|000101020212|NO 17 KAMPUNG BUKIT PETAI 08000 SUNGAI PETANI KEDAH                                                      |\n",
      "|000101020239|NO67 LORONG SEMARAK 1/2B TAMAN SEMARAK 08000 SUNGAI PETANI KEDAH                                         |\n",
      "|000101020255|NO 40, LORONG BUKIT PANCHOR 1, TAMAN BUKIT PANCHOR 14300 NIBONG TEBAL PULAU PINANG                       |\n",
      "|000101020263|C-1 LORONG 2 PERUMAHAN KILANG GULA 02500 KANGAR PERLIS                                                   |\n",
      "|000101020271|NO 96 PERMATANG TOK LABU 13100 PENAGA PULAU PINANG                                                       |\n",
      "|000101020300|252 LORONG 1/1 TAMAN RHU 1 TIKAM BATU 08600 SUNGAI PETANI KEDAH                                          |\n",
      "|000101020319|LOT 10 PUSAT PERNIGAAN KELIBANG KUAH 07000 LANGKAWI KEDAH                                                |\n",
      "|000101020335|2937 LORONG SERAMPANG 2/4, TAMAN RIA JAYA 08000 SUNGAI PETANI KEDAH                                      |\n",
      "+------------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessaery packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as spark_func\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# Some settings\n",
    "str_file = \"fix_Peka40STR2023.txt\"\n",
    "address_cols = [\"NoKPKIR\", \"Alamat\", \"Poskod\", \"Bandar\", \"Negeri\", \"state\",]\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplacePostalCode\").getOrCreate()\n",
    "\n",
    "# Read the csv file\n",
    "df = spark.read.options(delimiter=\"|\", header=True).csv(str_file)\n",
    "\n",
    "# Select the important columns for address, then cast string to postcode, capitalize address, city, state, create a column of state\n",
    "address_df = df.select(*address_cols[0:5])\\\n",
    "               .withColumn(\"Poskod\", df[\"Poskod\"].cast(\"string\"))\\\n",
    "               .withColumn(\"state\", spark_func.upper(\"Negeri\"))\n",
    "address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \"^#\", \"\"))\n",
    "\n",
    "# To capitalize all the address related information\n",
    "for column in address_cols[1:]:\n",
    "    address_df = address_df.withColumn(column, spark_func.upper(spark_func.trim(column)))\n",
    "               \n",
    "# Create list of state\n",
    "# ['W.PERSEKUTUAN (KL)', 'JOHOR', 'KEDAH', 'PERAK', 'PERLIS', 'PULAU PINANG', 'W.PERSEKUTUAN (LABUAN)', \n",
    "#  'SELANGOR', 'TERENGGANU', 'NEGERI SEMBILAN', 'KELANTAN', 'SARAWAK', 'W.PERSEKUTUAN (PUTRAJAYA)', 'PAHANG', 'MELAKA', 'SABAH']\n",
    "state_df = address_df.groupBy(\"Negeri\").count()\n",
    "state_list = [item[\"Negeri\"] for item in state_df.sort(\"Negeri\").toLocalIterator()]\n",
    "\n",
    "# Create another state column by changing federal states' name\n",
    "state_change_dict = {\"W.PERSEKUTUAN (KL)\":\"KUALA LUMPUR\",\n",
    "                     \"W.PERSEKUTUAN (LABUAN)\":\"LABUAN\",\n",
    "                     \"W.PERSEKUTUAN (PUTRAJAYA)\":\"PUTRAJAYA\"}\n",
    "\n",
    "# Change the state name\n",
    "for key in state_change_dict:\n",
    "    address_df = address_df.withColumn(\"state\", spark_func.when(spark_func.col(\"state\") == key, state_change_dict[key])\\\n",
    "                                       .otherwise(spark_func.col(\"state\")))\n",
    "\n",
    "for item in reversed(address_cols[2:]):\n",
    "    # To replace all poskod, city and state in the address\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], address_df[item], \"\"))\n",
    "    # To remove all those end with ,\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*$\", \"\"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*\", \", \"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*,\\\\s*\", \", \"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \"\\\\s*,\", \",\"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.trim(spark_func.regexp_replace(address_df[\"Alamat\"], \"\\\\s+\", \" \")))\n",
    "    \n",
    "address_df = address_df.withColumn(\"address\", spark_func.when(spark_func.col(\"Alamat\") != \"\", spark_func.concat_ws(\" \", *address_cols[1:4], \"state\")))\n",
    "address_df1 = address_df.filter(spark_func.col(\"address\").isNotNull())\n",
    "# address_df1.dropDuplicates([\"address\"])\n",
    "address_df1.select(address_cols[0], \"address\").show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1377.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# for value in reversed(address_cols[2:]):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# print(item)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# expr(f\"regexp_replace(Alamat, '{col_name}$', '')\"))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# df1 = address_df1.dropDuplicates([\"address\"]).select(address_cols[0], \"address\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# df1.show(truncate = False)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[43maddress_df1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress_cols\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maddress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstr_address\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1377.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# for value in reversed(address_cols[2:]):\n",
    "    # print(item)\n",
    "    # expr(f\"regexp_replace(Alamat, '{col_name}$', '')\"))\n",
    "    # spark_func.expr(spark_func.trim(address_df[\"Alamat\"]), address_df[item], \"\"))\n",
    "    # address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(spark_func.trim(address_df[\"Alamat\"]), f'\\\\b{item}\\\\b$', ''))\n",
    "    # address_df = address_df.withColumn(\"Alamat\", spark_func.expr(f\"CASE WHEN split(Alamat, ' ')[-1] = '{value}' THEN regexp_replace(Alamat, ' {value}$', '') ELSE Alamat END\"))\n",
    "\n",
    "# df1 = address_df1.dropDuplicates([\"address\"]).select(address_cols[0], \"address\")\n",
    "# df1.show(truncate = False)\n",
    "address_df1.select(address_cols[0], \"address\")\\\n",
    "    .write.format(\"csv\").mode('overwrite').options(header='True', delimiter='|')\\\n",
    "        .save(\"str_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------------------------------+------+------------+------+-----+\n",
      "|NoKPKIR     |Alamat                                                                   |Poskod|Bandar      |Negeri|state|\n",
      "+------------+-------------------------------------------------------------------------+------+------------+------+-----+\n",
      "|000101010049|LOT 8091, KAMPUNG SENGKANG BATU 18 BUKIT GAMBIR TANGKAK                  |84800 |BUKIT GAMBIR|JOHOR |JOHOR|\n",
      "|000101010057|NO 21 A PARIT AMAL LORONG HAJI SUHAIMI JALAN SRI MUDA                    |84000 |MUAR        |JOHOR |JOHOR|\n",
      "|000101010073|NO34 TINGKAT 2 BLOCK B TAMAN DESA RAKYAT PERDANA PASIR GUDANG            |81700 |PASIR GUDANG|JOHOR |JOHOR|\n",
      "|000101010081|NO 13, JALAN TERKUKUR, LARKIN JAYA                                       |80350 |JOHOR BAHRU |JOHOR |JOHOR|\n",
      "|000101010102|BLOK A4, 02-10 JALAN SEROJA INDAH 2 TAMAN SEROJA                         |81200 |JOHOR BAHRU |JOHOR |JOHOR|\n",
      "|000101010110|NO 31 JALAN SIERRA PERDANA 1/5 TAMAN SIERRA PERDANA                      |81750 |MASAI       |JOHOR |JOHOR|\n",
      "|000101010129|20/3 LORONG A4 JALAN BUNGA RAYA KAMPUNG MELAYU                           |86000 |KLUANG      |JOHOR |JOHOR|\n",
      "|000101010137|NO 37-C KAMPUNG PARIT BUGIS SENGGARANG BATU PAHAT                        |83200 |SENGGARANG  |JOHOR |JOHOR|\n",
      "|000101010145|6 JALAN RAMUNIA 4 DESA RAMUNIA                                           |81620 |PENGERANG   |JOHOR |JOHOR|\n",
      "|000101010153|NO6 TAMAN PASIR RAYA BATU6 1/4 BUKIT PASIR BUKIT PASIR                   |84300 |BUKIT PASIR |JOHOR |JOHOR|\n",
      "|000101010196|NO 36, LORONG 10, JALAN 8/19, PERJIRANAN 8, TAMAN AIR BIRU, PASIR GUDANG |81700 |PASIR GUDANG|JOHOR |JOHOR|\n",
      "|000101010217|NO 6 JALAN PADI MAHSURI 6 BANDAR BARU UDA                                |81200 |JOHOR BAHRU |JOHOR |JOHOR|\n",
      "|000101010225|POS 123, TL 191, KG SG GERSIK DARAT SEMERAH BATU PAHAT                   |83600 |SEMERAH     |JOHOR |JOHOR|\n",
      "|000101010241|NO 3 JALAN DELIMA 1 TAMAN DELIMA 1/2 81100 JOHOR BAHRU                   |81100 |JOHOR BAHRU |JOHOR |JOHOR|\n",
      "|000101010268|NO 2 JALAN MATAHARI 4 TAMAN MATAHARI 1                                   |86000 |KLUANG      |JOHOR |JOHOR|\n",
      "|000101010356|15 JALAN LEMBAH 21 BANDAR SERI ALAM 81750 MASAI                          |81750 |MASAI       |JOHOR |JOHOR|\n",
      "|000101010364|NO. 146, JALAN SONGKET 2, TAMAN ANJARIA 81000, KULAI                     |81000 |KULAI       |JOHOR |JOHOR|\n",
      "|000101010399|NO 84, LORONG LASSIM, JALAN HAJI ABDULLAH, 84000 MUAR, JOHOR.            |84000 |MUAR        |JOHOR |JOHOR|\n",
      "|000101010401|NO 9 JALAN PULAI JAYA 2/7 BANDAR BARU PULAI JAYA 81300 SKUDAI JOHOR BAHRU|81300 |JOHOR BAHRU |JOHOR |JOHOR|\n",
      "|000101010436|J80, KAMPUNG JALAN RAJA BUKIT PASIR 84300, MUAR                          |84300 |BUKIT PASIR |JOHOR |JOHOR|\n",
      "+------------+-------------------------------------------------------------------------+------+------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_address(df, columns_to_loop, column_need_to_be_clean,):\n",
    "    for value in columns_to_loop:\n",
    "        df = df.withColumn(\"words\", spark_func.split(column_need_to_be_clean, \"\\\\s+\"))\\\n",
    "               .withColumn(\"last\", spark_func.expr(\"words[size(words) - 1]\"))\\\n",
    "               .withColumn(\"words\", spark_func.expr(\"slice(words, 1, size(words) - 1)\"))\n",
    "        df = df.withColumn(\"last\", spark_func.when(df[value]==df[\"last\"], \"\").otherwise(df[\"last\"]))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.trim(spark_func.concat_ws(\" \", df[\"words\"], df[\"last\"])))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*$\", \"\"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*\", \", \"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*,\\\\s*\", \", \"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \"\\\\s*,\", \",\"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.trim(spark_func.regexp_replace(df[column_need_to_be_clean], \"\\\\s+\", \" \")))\n",
    "        return df.drop(\"words\", \"last\")\n",
    "clean_address(address_df, reversed(address_cols[2:]), \"Alamat\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
