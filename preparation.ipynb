{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade geopy ipykernel matplotlib numpy openpyxl pandas pip plotly_express polars PyCap python-dotenv pyspark seaborn setuptools tabulate tabula-py\n",
    "# %pip install pandas==1.5.3 [pandas_on_spark]\n",
    "# %pip install distutils\n",
    "# %pip install pyarrow\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessaery packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as spark_func\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# Some settings\n",
    "str_file = \"fix_Peka40STR2023.txt\"\n",
    "address_cols = [\"NoKPKIR\", \"Alamat\", \"Poskod\", \"Bandar\", \"Negeri\", \"state\",]\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplacePostalCode\").getOrCreate()\n",
    "\n",
    "# Read the csv file\n",
    "df = spark.read.options(delimiter=\"|\", header=True).csv(str_file)\n",
    "\n",
    "# Select the important columns for address, then cast string to postcode, capitalize address, city, state, create a column of state\n",
    "address_df = df.select(*address_cols[0:5])\\\n",
    "               .withColumn(\"Poskod\", df[\"Poskod\"].cast(\"string\"))\\\n",
    "               .withColumn(\"state\", spark_func.upper(\"Negeri\"))\n",
    "# address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \"^#\", \"\"))\n",
    "\n",
    "# To capitalize all the address related information\n",
    "for column in address_cols[1:]:\n",
    "    address_df = address_df.withColumn(column, spark_func.upper(spark_func.trim(column)))\n",
    "               \n",
    "# Create list of state\n",
    "# ['W.PERSEKUTUAN (KL)', 'JOHOR', 'KEDAH', 'PERAK', 'PERLIS', 'PULAU PINANG', 'W.PERSEKUTUAN (LABUAN)', \n",
    "#  'SELANGOR', 'TERENGGANU', 'NEGERI SEMBILAN', 'KELANTAN', 'SARAWAK', 'W.PERSEKUTUAN (PUTRAJAYA)', 'PAHANG', 'MELAKA', 'SABAH']\n",
    "state_df = address_df.groupBy(\"Negeri\").count()\n",
    "state_list = [item[\"Negeri\"] for item in state_df.sort(\"Negeri\").toLocalIterator()]\n",
    "\n",
    "# Create another state column by changing federal states' name\n",
    "state_change_dict = {\"W.PERSEKUTUAN (KL)\":\"KUALA LUMPUR\",\n",
    "                     \"W.PERSEKUTUAN (LABUAN)\":\"LABUAN\",\n",
    "                     \"W.PERSEKUTUAN (PUTRAJAYA)\":\"PUTRAJAYA\"}\n",
    "\n",
    "# Change the state name\n",
    "for key in state_change_dict:\n",
    "    address_df = address_df.withColumn(\"state\", spark_func.when(spark_func.col(\"state\") == key, state_change_dict[key])\\\n",
    "                                       .otherwise(spark_func.col(\"state\")))\n",
    "\n",
    "for item in reversed(address_cols[2:]):\n",
    "    # To replace all poskod, city and state in the address\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], address_df[item], \"\"))\n",
    "    # To remove all those end with ,\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*$\", \"\"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*\", \", \"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \",\\\\s*,\\\\s*\", \", \"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(address_df[\"Alamat\"], \"\\\\s*,\", \",\"))\n",
    "    address_df = address_df.withColumn(\"Alamat\", spark_func.trim(spark_func.regexp_replace(address_df[\"Alamat\"], \"\\\\s+\", \" \")))\n",
    "    \n",
    "address_df = address_df.withColumn(\"address\", spark_func.when(spark_func.col(\"Alamat\") != \"\", spark_func.concat_ws(\" \", *address_cols[1:4], \"state\")))\n",
    "address_df1 = address_df.filter(spark_func.col(\"address\").isNotNull())\n",
    "# address_df1.dropDuplicates([\"address\"])\n",
    "address_df1.select(address_cols[0], \"address\").show(100, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for value in reversed(address_cols[2:]):\n",
    "    # print(item)\n",
    "    # expr(f\"regexp_replace(Alamat, '{col_name}$', '')\"))\n",
    "    # spark_func.expr(spark_func.trim(address_df[\"Alamat\"]), address_df[item], \"\"))\n",
    "    # address_df = address_df.withColumn(\"Alamat\", spark_func.regexp_replace(spark_func.trim(address_df[\"Alamat\"]), f'\\\\b{item}\\\\b$', ''))\n",
    "    # address_df = address_df.withColumn(\"Alamat\", spark_func.expr(f\"CASE WHEN split(Alamat, ' ')[-1] = '{value}' THEN regexp_replace(Alamat, ' {value}$', '') ELSE Alamat END\"))\n",
    "\n",
    "# df1 = address_df1.dropDuplicates([\"address\"]).select(address_cols[0], \"address\")\n",
    "# df1.show(truncate = False)\n",
    "address_df1.select(address_cols[0], \"address\")\\\n",
    "    .coalesce(1).write.format(\"csv\").mode('overwrite').options(header='True', delimiter='|')\\\n",
    "        .save(\"str_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(df, columns_to_loop, column_need_to_be_clean,):\n",
    "    for value in columns_to_loop:\n",
    "        df = df.withColumn(\"words\", spark_func.split(column_need_to_be_clean, \"\\\\s+\"))\\\n",
    "               .withColumn(\"last\", spark_func.expr(\"words[size(words) - 1]\"))\\\n",
    "               .withColumn(\"words\", spark_func.expr(\"slice(words, 1, size(words) - 1)\"))\n",
    "        df = df.withColumn(\"last\", spark_func.when(df[value]==df[\"last\"], \"\").otherwise(df[\"last\"]))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.trim(spark_func.concat_ws(\" \", df[\"words\"], df[\"last\"])))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*$\", \"\"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*\", \", \"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \",\\\\s*,\\\\s*\", \", \"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.regexp_replace(df[column_need_to_be_clean], \"\\\\s*,\", \",\"))\n",
    "        df = df.withColumn(column_need_to_be_clean, spark_func.trim(spark_func.regexp_replace(df[column_need_to_be_clean], \"\\\\s+\", \" \")))\n",
    "        return df.drop(\"words\", \"last\")\n",
    "clean_address(address_df, reversed(address_cols[2:]), \"Alamat\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessaery packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as spark_func\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# Some settings\n",
    "str_file = \"fix_Peka40STR2023.txt\"\n",
    "address_cols = [\"NoKPKIR\", \"Alamat\", \"Poskod\", \"Bandar\", \"Negeri\", \"state\",]\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"ReplacePostalCode\").getOrCreate()\n",
    "\n",
    "# Read the csv file\n",
    "df = spark.read.options(delimiter=\"|\", header=True).csv(\"str_address\")\n",
    "df = df.pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.ExcelWriter(\"str_address.xlsx\", mode=\"w\") as writer:\n",
    "#     for num in range(0, len(df), 1000):\n",
    "#         df.iloc[num:num+1000].to_excel(writer, sheet_name = f\"sheet{num/1000}\", index = False)\n",
    "\n",
    "for num in range(0, len(df), 1000):\n",
    "    df.iloc[num:num+1000].to_excel(f\"str_partition/sheet{num/1000}.xlsx\", sheet_name = f\"sheet{num//1000}\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/tabula/io.py:1045: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tabula\n",
    "file_path = \"/Users/wh0102/Downloads/github/7046_mphd/MGT GROUPING 23.24.pdf\"\n",
    "df = tabula.read_pdf(file_path, pages = \"all\", lattice = False, stream = False)\n",
    "df[1].to_clipboard(index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
