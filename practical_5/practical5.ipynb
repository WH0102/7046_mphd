{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2c32af-996f-4304-8745-d70ae9c67e52",
   "metadata": {},
   "source": [
    "## MQB7046 MODELLING PUBLIC HEALTH DATA - Ordinal logistic regression\n",
    "Prepared by Claire Choo (15/4/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb216d-f366-4865-a89a-c508d66a50bc",
   "metadata": {},
   "source": [
    "Ordinal Logistic Regression is a statistical model used when the dependent variable is ordinal in nature, that is it has a clear ordering or ranking, but the differences between the categories may not be equal. \n",
    "The dependent variable must be ordinal, such as a rating scale (e.g. 1 = poor, 2 = fair, 3 = good, 4 = excellent). \n",
    "The model assumes that the relationship between the predictor variables and the logit of the cumulative probabilities is the same for all logits (the proportional odds assumption). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c7c5c-fa2c-499b-a1e2-bd384e51b37e",
   "metadata": {},
   "source": [
    "#### Practical 5\n",
    "\n",
    "A study examined factors that influence the decision of whether to apply to graduate programme.  Undergraduates were asked if they are unlikely, somewhat likely, or very likely to apply to graduate programme. The outcome variable has three categories coded as 0,1,2. Data on parental educational status (0 is no tertiary education, 1 is with tertiary education), whether the undergraduate institution is public (1) or private (0), and current GPA was collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f31be6-c001-45f1-8cab-6528b97fc5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+--------------------+-------------+\n",
      "| id | Sex  |  Age  |      Marital       | Employment  |\n",
      "+----+------+-------+--------------------+-------------+\n",
      "| 1  | Male | 55-64 | Married/Cohabiting |  Employed   |\n",
      "| 2  | Male | 25-44 | Married/Cohabiting | Nonemployed |\n",
      "| 3  | Male | 25-44 | Married/Cohabiting |  Employed   |\n",
      "| 4  | Male | 55-64 | Married/Cohabiting |  Employed   |\n",
      "| 5  | Male | 55-64 | Married/Cohabiting |  Employed   |\n",
      "+----+------+-------+--------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "grad = pd.read_csv(\"open.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify that the data is loaded correctly\n",
    "print(grad.head().to_markdown(tablefmt = \"pretty\", index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4444e282-270c-4818-b398-8d023e77c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai.llm import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "df = SmartDataframe(grad, config = {'llm': llm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03514e64-8f05-46c3-bff1-7c821d41eb73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/pipelines/chat/generate_chat_pipeline.py\", line 283, in run\n",
      "    output = (self.code_generation_pipeline | self.code_execution_pipeline).run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/pipelines/pipeline.py\", line 137, in run\n",
      "    raise e\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/pipelines/pipeline.py\", line 101, in run\n",
      "    step_output = logic.execute(\n",
      "                  ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/pipelines/chat/code_generator.py\", line 33, in execute\n",
      "    code = pipeline_context.config.llm.generate_code(input, pipeline_context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/llm/base.py\", line 196, in generate_code\n",
      "    response = self.call(instruction, context)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/llm/base.py\", line 386, in call\n",
      "    self.chat_completion(self.last_prompt, memory)\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/pandasai/llm/base.py\", line 360, in chat_completion\n",
      "    response = self.client.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 581, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1233, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 922, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 998, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1046, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 998, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1046, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/wh0102/Downloads/github/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1013, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, I was not able to answer your question, because of the following error:\\n\\nError code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.chat(\"Perform a descriptive statistics analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31eddb-21a2-4a07-88dc-fb85b209aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the descriptive statistics \n",
    "# Conduct hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b351a8-3c43-4ec4-8c23-b47dd8775d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate odds ratio (95%CI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380be1f2-cdcd-4236-95a6-760c4ea16dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ordinal logistic regression with multiple predictors ('pared', 'public', 'gpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98f2e3-6a1a-4900-8996-474d74e45f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predicted probabilities for each observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c224432-1785-4d9d-a17b-0497cdbbb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13dc6d-dd03-4250-907c-8dc940c12ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the likelihood ratio test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a960d-e652-49a0-a2a4-b7304c3403bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform score test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00918e8-b03a-4aa6-8e77-ece468cf3dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708b3168-f338-4d19-b538-74a93c02b39b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade27441-09bd-46ef-8f7b-a12836e97ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae63f7-e8e0-4943-a957-458e319e56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f9c54-7f57-4dee-841d-4e61524195a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca994c42-7452-4c43-98d4-c28d25a5b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85ecf6-c80b-4c03-af30-75dbc60fe621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
