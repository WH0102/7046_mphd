{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip FLAML \"flaml[spark]\" shap setuptools wheel optuna optuna-integration openml xgboost catboost imbalanced-learn pandas scipy statsmodels\n",
    "# %pip install --upgrade \"scikit-learn==1.1.2\"\n",
    "# %pip install catboost FLAML \"flaml[spark]\" geopy gspread imbalanced-learn ipykernel matplotlib numpy \\\n",
    "#     openpyxl openai openml optuna optuna-integration \\\n",
    "#         pandas pip plotly_express polars PyCap pygsheets python-dotenv pyspark \\\n",
    "#             seaborn scipy setuptools scikit-learn shap statsmodels tabulate tabula-py wheel xlsx2csv xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow own package to be imported\n",
    "import sys\n",
    "import os\n",
    "if os.path.dirname(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action = \"ignore\")\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import wh0102 as mphd\n",
    "\n",
    "# Prepare the data dictionary\n",
    "data_dictionary = {\n",
    "    \"Ethnic\":{0:\"Malay\", 1:\"Chinese\", 2:\"Indian\"},\n",
    "    \"bmi\":{0:\"Normal BMI\", 1:\"Overweight\"},\n",
    "    \"Disease\":{0:\"No liver disease\", 1:\"Have Liver Disease\"},\n",
    "    \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "}\n",
    "\n",
    "normal_values = {\"TP\":[64, 83],\n",
    "                 \"ALB\":[35, 52],\n",
    "                 \"TB\":[0, 22],\n",
    "                 \"ALP\":[40, 130],\n",
    "                 \"ALT\":[0, 42],\n",
    "                 \"AST\":[0, 41]}\n",
    "\n",
    "# Rename for easier references\n",
    "column_to_be_rename = {\"Sgot\":\"ALT\",\n",
    "                       \"Sgpt\":\"AST\",\n",
    "                       \"Alkphos\":\"ALP\"}\n",
    "\n",
    "# Prepare the variables\n",
    "dependent_variable = \"Disease\"\n",
    "independent_demographic = (\"Age\", \"Gender\", \"Ethnic\", \"bmi\",)\n",
    "independent_investigations = (\"AGR\", \"ALB\", \"TP\", \"TB\", \"DB\", \"ALP\", \"ALT\", \"AST\",)\n",
    "independent_continous = (independent_demographic[0],) + independent_investigations\n",
    "independent_categorical = independent_demographic[1:]\n",
    "independent_variables = independent_demographic + independent_investigations\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r\"assignment4.csv\")\n",
    "\n",
    "# Rename the column name\n",
    "df = df.rename(columns = column_to_be_rename)\n",
    "\n",
    "# To reassign the categorical value\n",
    "for column in [key for key in data_dictionary.keys() if key != \"Gender\"]:\n",
    "    df.loc[:,column] = df.loc[:,column] - 1\n",
    "\n",
    "# To change 0 & 1 for the bmi and disease in reverse order\n",
    "for column in [\"bmi\", \"Disease\"]:\n",
    "    df.loc[:,column] = df.loc[:,column].replace({0:1, 1:0})\n",
    "\n",
    "# Print the information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the gender\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "# \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "\n",
    "# Check for duplication\n",
    "duplicated_df, to_drop_duplicated_df = mphd.pre_processing.check_duplication(df)\n",
    "\n",
    "# Check for missing value\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To fix AGR == ' ' issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Globulin for every patient ID with globulin = tp - alb based on resource below:\n",
    "# https://www.ncbi.nlm.nih.gov/books/NBK204/#:~:text=The%20total%20globulin%20fraction%20is,of%20further%20fractionating%20serum%20proteins\n",
    "\n",
    "# To check the truthness of this on the data\n",
    "# Create a deep copy of the df with AGR not null first\n",
    "temp_df = df.query(\"AGR.notnull()\").copy(deep = True)\n",
    "\n",
    "# Calculatet the globulin and agr_ratio\n",
    "def calculate_agr(df:pd.DataFrame, column_name:str):\n",
    "    df.loc[:,column_name] = df.loc[:,\"ALB\"] / (df.loc[:,\"TP\"] - df.loc[:,\"ALB\"])\n",
    "    return df\n",
    "\n",
    "# Calculate the approximate agr\n",
    "temp_df = calculate_agr(df = temp_df, column_name = \"agr_new\")\n",
    "# Check for float similarity\n",
    "temp_df.loc[:,\"agr_similarity\"] = temp_df.loc[:,(\"AGR\", \"agr_new\",)]\\\n",
    "    .apply(lambda x: np.isclose(float(x[0]), x[1], rtol = 0.1), axis = 1)\n",
    "\n",
    "# Pivot the information\n",
    "pt = temp_df.pivot_table(index = \"agr_similarity\", values = \"Patient_ID\", aggfunc = len, margins = True)\\\n",
    "    .rename(columns={\"Patient_ID\":\"count\"})\n",
    "# Calculate percentage\n",
    "pt.loc[:,\"percentage\"] = round(pt.loc[:,\"count\"] / pt.loc[\"All\", \"count\"] * 100, 2)\n",
    "\n",
    "print(pt.to_markdown(tablefmt = \"pretty\"))\n",
    "\n",
    "# Check on the not similarity result\n",
    "temp_df.query(\"agr_similarity == False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial to impute with calculation\n",
    "missing_df_temp = calculate_agr(df = missing_df, column_name=\"AGR\")\n",
    "missing_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with MissingForest\n",
    "# https://betterdatascience.com/python-missforest-algorithm/#google_vignette\n",
    "\n",
    "# mf_df = mphd.missing_values.miss_forest_imputation(df=df, columns = \"AGR\")\n",
    "# # Check on the imputated value\n",
    "# mf_df.loc[missing_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with MICE\n",
    "# https://medium.com/@brijesh_soni/topic-9-mice-or-multivariate-imputation-with-chain-equation-f8fd435ca91#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,produce%20a%20final%20imputed%20dataset.\n",
    "\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")\n",
    "# Check on the imputated value\n",
    "df.loc[missing_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__intepretation__:\n",
    "\n",
    "For imputation, despite the logic of how AGR being calculated, there is a lot of noise in the data for AGR value, therefore we would use MICE for imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normal distribution\n",
    "# normal_distribution_list, abnormal_distribution_list = mphd.continous_data.descriptive_analysis(df = df, \n",
    "#                                                                                                 independent_variables=independent_continous, \n",
    "#                                                                                                 dependent_variables = dependent_variable,\n",
    "#                                                                                                 descriptive_type = \"continous\",\n",
    "#                                                                                                 plot_dependent_variables = False,\n",
    "#                                                                                                 plot_correlation = True, \n",
    "#                                                                                                 round = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outliers with 1.5 * iqr\n",
    "# outliers_df = mphd.continous_data.identify_outliers(df = df, \n",
    "#                                                     column_name = [\"TP\", \"TB\", \"ALP\", \"AST\", \"ALT\"], \n",
    "#                                                     ratio = 1.5,\n",
    "#                                                     normal_values = normal_values)\n",
    "\n",
    "# normal_distribution_list, abnormal_distribution_list = mphd.continous_data.descriptive_analysis(df = df.loc[~df.index.isin(outliers_df.index)], \n",
    "#                                                                                                 independent_variables=independent_continous, \n",
    "#                                                                                                 dependent_variables = dependent_variable,\n",
    "#                                                                                                 descriptive_type = \"continous\",\n",
    "#                                                                                                 plot_dependent_variables = False,\n",
    "#                                                                                                 plot_correlation = True, \n",
    "#                                                                                                 round = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Encode\n",
    "# data = mphd.categorical_data.reverse_encode(df = df, json_dict=data_dictionary)\n",
    "\n",
    "# # Categegorical Data Analysis\n",
    "# categorical_summary = mphd.categorical_data.categorical_descriptive_analysis(data,\n",
    "#                                                                              independent_variables = independent_categorical, \n",
    "#                                                                              dependent_variables = dependent_variable, \n",
    "#                                                                              analyse_dependent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = mphd.pre_processing.train_test_split(df = df,\n",
    "                                                                        independent_variables=independent_variables,\n",
    "                                                                        dependent_variable = dependent_variable,\n",
    "                                                                        test_size = 0.2)\n",
    "\n",
    "# Generate random_seed\n",
    "random_seed = 168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the params\n",
    "# logistic_regression_params = {'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#                               'classifier__C': np.logspace(0, 4, 10),\n",
    "#                               'classifier__solver': ['liblinear'],\n",
    "#                               'classifier__max_iter': [100, 200, 300]}\n",
    "\n",
    "# # Build logistic regression\n",
    "# logistic_regression_grid_search, logistic_regression_time_required = mphd.machine_learning.logisticRegression(X_train = X_train, \n",
    "#                                                                                                               y_train = y_train,\n",
    "#                                                                                                               params = logistic_regression_params, \n",
    "#                                                                                                               random_seed = random_seed)\n",
    "# # Perform analysis on Random Forest\n",
    "# lr_summary_df = mphd.analyse_ml.analyse_ml(logistic_regression_grid_search,\n",
    "#                                            time_required = logistic_regression_time_required,\n",
    "#                                            model_type = \"Logistic Regression\",\n",
    "#                                            independent_variables=independent_variables,\n",
    "#                                            X_test=X_test, y_test=y_test)\n",
    "# lr_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM params\n",
    "# svm_params=param_grid = {\n",
    "#     'classifier__C': [0.1, 1, 10, 100],\n",
    "#     'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'classifier__gamma': [0.1, 0.01, 0.001],\n",
    "#     'classifier__degree': [3, 4, 5]\n",
    "# }\n",
    "\n",
    "# svm_grid_search, svm_time_required = mphd.machine_learning.svm(X_train = X_train, \n",
    "#                                                                y_train = y_train,\n",
    "#                                                                params = svm_params, \n",
    "#                                                                random_seed = random_seed)\n",
    "# # Perform analysis on Random Forest\n",
    "# svm_summary_df = mphd.analyse_ml.analyse_ml(svm_grid_search,\n",
    "#                                            time_required = svm_time_required,\n",
    "#                                            model_type = \"SVM\",\n",
    "#                                            independent_variables=independent_variables,\n",
    "#                                            X_test=X_test, y_test=y_test)\n",
    "# svm_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour(kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# knn_params = {\n",
    "#     'classifier__n_neighbors': [3, 5, 7],\n",
    "#     'classifier__weights': ['uniform', 'distance'],\n",
    "#     'classifier__metric': ['euclidean', 'manhattan', 'chebyshev']\n",
    "# }\n",
    "\n",
    "# # Build logistic regression\n",
    "# knn_grid_search, knn_time_required = mphd.machine_learning.knn(X_train = X_train, \n",
    "#                                                                y_train = y_train,\n",
    "#                                                                params = knn_params, \n",
    "#                                                                random_seed = random_seed)\n",
    "# # Perform analysis on Random Forest\n",
    "# knn_summary_df = mphd.analyse_ml.analyse_ml(knn_grid_search,\n",
    "#                                            time_required = knn_time_required,\n",
    "#                                            model_type = \"kNN\",\n",
    "#                                            independent_variables=independent_variables,\n",
    "#                                            X_test=X_test, y_test=y_test)\n",
    "# knn_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# # 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'monotonic_cst', 'random_state', 'splitter'\n",
    "# decision_tree_params = {\n",
    "#     'classifier__criterion': ['gini', 'entropy'],\n",
    "#     'classifier__max_depth': [None, 5, 10, 20],\n",
    "#     'classifier__min_samples_split': [2, 5, 10],\n",
    "#     'classifier__min_samples_leaf': [1, 2, 4],\n",
    "#     'classifier__max_features':[0.8],\n",
    "#     'classifier__class_weight': ['balanced', None]\n",
    "# }\n",
    "\n",
    "# # Build logistic regression\n",
    "# decision_tree_grid_search, decision_tree_time_required = mphd.machine_learning.decision_tree(X_train = X_train, \n",
    "#                                                                                              y_train = y_train,\n",
    "#                                                                                              params = decision_tree_params, \n",
    "#                                                                                              random_seed = random_seed)\n",
    "# # Perform analysis on Random Forest\n",
    "# decision_tree_summary_df = mphd.analyse_ml.analyse_ml(decision_tree_grid_search,\n",
    "#                                            time_required = decision_tree_time_required,\n",
    "#                                            model_type = \"Decision Tree\",\n",
    "#                                            independent_variables=independent_variables,\n",
    "#                                            X_test=X_test, y_test=y_test)\n",
    "# decision_tree_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Ramdon Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare Random Forest Params\n",
    "# rf_params = {'classifier__max_depth':[7],\n",
    "#               'smote__sampling_strategy': [0.8],\n",
    "#               'classifier__min_samples_split':[10],\n",
    "#               'classifier__max_features':[0.8],\n",
    "#               'classifier__criterion':[\"entropy\"],\n",
    "#               'classifier__bootstrap':[True],\n",
    "#               'classifier__n_estimators':[200],\n",
    "#               'classifier__min_samples_leaf':[8]}\n",
    "\n",
    "# random_forest_grid_search, random_forest_time_required = mphd.machine_learning.random_forest(X_train = X_train, \n",
    "#                                                                                              y_train = y_train,\n",
    "#                                                                                              params = rf_params, \n",
    "#                                                                                              random_seed = random_seed)\n",
    "\n",
    "# # Perform analysis on Random Forest\n",
    "# rf_summary_df = mphd.analyse_ml.analyse_ml(random_forest_grid_search,\n",
    "#                                            time_required = random_forest_time_required,\n",
    "#                                            model_type = \"Random Forest\",\n",
    "#                                            independent_variables=independent_variables,\n",
    "#                                            X_test=X_test, y_test=y_test)\n",
    "# rf_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Bboost(XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "xgb_params = {\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'classifier__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    'classifier__n_estimators': [100, 150, 200, 500, 1000],\n",
    "    'classifier__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__gamma': [0, 0.25, 0.5, 1.0],\n",
    "    'classifier__min_child_weight': [1, 3, 5, 7],\n",
    "    'classifier__scale_pos_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "xgb_grid_search, xbg_time_required = mphd.machine_learning.xgb(X_train = X_train, \n",
    "                                                               y_train = y_train,\n",
    "                                                               params = xgb_params, \n",
    "                                                              random_seed = random_seed)\n",
    "\n",
    "# Perform analysis on Random Forest\n",
    "xgb_summary_df = mphd.analyse_ml.analyse_ml(xgb_grid_search,\n",
    "                                           time_required = xbg_time_required,\n",
    "                                           model_type = \"XGB\",\n",
    "                                           independent_variables=independent_variables,\n",
    "                                           X_test=X_test, y_test=y_test)\n",
    "xgb_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Light Gradient Boost Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LightGBM\n",
    "# lgbm_params = {'classifier__max_depth': [3,4,5],\n",
    "#                'classifier__learning_rate' : [0.1, 0.2],\n",
    "#                'classifier__min_child_weight' : range(1,3,1),\n",
    "#                'classifier__boosting_type' : ['gbdt'],\n",
    "#                'smote__sampling_strategy': np.linspace(0.5, 0.9, 2),\n",
    "#                'classifier__reg_alpha':[1e-5,0.01,0.03],\n",
    "#                'classifier__num_leaves':[6]}\n",
    "\n",
    "# LightGBM_grid_search, LightGBM_time_required = mphd.machine_learning.LightGBM(X_train = X_train, \n",
    "#                                                                               y_train = y_train,\n",
    "#                                                                               params = lgbm_params,\n",
    "#                                                                               independent_variables_continous = independent_continous,\n",
    "#                                                                               random_seed = random_seed)\n",
    "\n",
    "# # Perform analysis on LightGBM\n",
    "# lightgbm_summary_df = mphd.analyse_ml.analyse_ml(LightGBM_grid_search,\n",
    "#                                                  time_required = LightGBM_time_required,\n",
    "#                                                  model_type = \"LightGBM\",\n",
    "#                                                  independent_variables=independent_variables,\n",
    "#                                                  X_test=X_test, y_test=y_test)\n",
    "# lightgbm_summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
