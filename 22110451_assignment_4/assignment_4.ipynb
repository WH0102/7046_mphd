{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip \"flaml[spark]\" setuptools wheel \"scikit-learn==1.1.2\"\n",
    "# %pip install --upgrade FLAML optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data detected for columns AGR.\n",
      "Summary of the missing values from the dataframe =\n",
      "+------------------------------+-------+--------------------+\n",
      "|                              | count | missing_percentage |\n",
      "+------------------------------+-------+--------------------+\n",
      "|             AGR              |  4.0  |        0.72        |\n",
      "| All_rows_with_missing_values |  4.0  |        0.72        |\n",
      "+------------------------------+-------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# To allow own package to be imported\n",
    "import sys\n",
    "import os\n",
    "if os.path.dirname(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action = \"ignore\")\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import wh0102 as mphd\n",
    "\n",
    "# Prepare the data dictionary\n",
    "data_dictionary = {\n",
    "    \"Ethnic\":{0:\"Malay\", 1:\"Chinese\", 2:\"Indian\"},\n",
    "    \"bmi\":{0:\"Normal BMI\", 1:\"Overweight\"},\n",
    "    \"Disease\":{0:\"No liver disease\", 1:\"Have Liver Disease\"},\n",
    "    \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "}\n",
    "\n",
    "# Prepare the variables\n",
    "dependent_variable = \"Disease\"\n",
    "independent_demographic = (\"Age\", \"Gender\", \"Ethnic\", \"bmi\",)\n",
    "independent_investigations = (\"AGR\", \"ALB\", \"TP\", \"TB\", \"DB\", \"Alkphos\", \"Sgot\", \"Sgpt\",)\n",
    "independent_continous = (independent_demographic[0],) + independent_investigations\n",
    "independent_categorical = independent_demographic[1:]\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r\"assignment4.csv\")\n",
    "\n",
    "# To reassign the categorical value\n",
    "for column in [key for key in data_dictionary.keys() if key != \"Gender\"]:\n",
    "    df.loc[:,column] = df.loc[:,column] - 1\n",
    "\n",
    "# Print the information\n",
    "# df.info()\n",
    "\n",
    "# To delete after this\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 05-25 15:59:11] {1605} WARNING - n_concurrent_trials > 1 is only supported when using Ray or Spark. Spark installed, setting use_spark to True. If you want to use Ray, set use_ray to True.\n",
      "[flaml.automl.logger: 05-25 15:59:11] {1680} INFO - task = regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 05-25 15:59:11] {1691} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 05-25 15:59:11] {1789} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl.logger: 05-25 15:59:11] {1901} INFO - List of ML learners in AutoML Run: ['lgbm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed a `space` parameter to OptunaSearch that contained unresolved search space definitions. OptunaSearch should however be instantiated with fully configured search spaces only. To use Ray Tune's automatic search space conversion, pass the space definition as part of the `config` argument to `tune.run()` instead.\n",
      "[I 2024-05-25 15:59:11,997] A new study created in memory with name: optuna\n",
      "[I 2024-05-25 15:59:12,057] A new study created in memory with name: optuna\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'joblibspark'. Try pip install flaml[spark] or set use_spark=False.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\flaml\\tune\\tune.py:659\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(evaluation_function, config, low_cost_partial_config, cat_hp_cost, metric, mode, time_budget_s, points_to_evaluate, evaluated_rewards, resource_attr, min_resource, max_resource, reduction_factor, scheduler, search_alg, verbose, local_dir, num_samples, resources_per_trial, config_constraints, metric_constraints, max_failure, use_ray, use_spark, use_incumbent_result_in_evaluation, log_file_name, lexico_objectives, force_cancel, n_concurrent_trials, **ray_args)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed, parallel_backend\n\u001b[1;32m--> 659\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblibspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_spark\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'joblibspark'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m      4\u001b[0m settings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_budget\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# total running time in seconds\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_concurrent_trials\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# the maximum number of concurrent trials\u001b[39;00m\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     14\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(df\u001b[38;5;241m.\u001b[39mloc[:,independent_demographic\u001b[38;5;241m+\u001b[39mindependent_investigations],\n\u001b[0;32m     15\u001b[0m                                                     df\u001b[38;5;241m.\u001b[39mloc[:,dependent_variable],\n\u001b[0;32m     16\u001b[0m                                                     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     17\u001b[0m                                                     stratify\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[:,dependent_variable],\n\u001b[0;32m     18\u001b[0m                                                     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\flaml\\automl\\automl.py:1926\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[1;34m(self, X_train, y_train, dataframe, label, metric, task, n_jobs, log_file_name, estimator_list, time_budget, max_iter, sample, ensemble, eval_method, log_type, model_history, split_ratio, n_splits, log_training_metric, mem_thres, pred_time_limit, train_time_limit, X_val, y_val, sample_weight_val, groups_val, groups, verbose, retrain_full, split_type, learner_selector, hpo_method, starting_points, seed, n_concurrent_trials, keep_search_state, preserve_checkpoint, early_stop, force_cancel, append_log, auto_augment, min_sample_size, use_ray, use_spark, free_mem_ratio, metric_constraints, custom_hp, time_col, cv_score_agg_func, skip_transform, mlflow_logging, fit_kwargs_by_estimator, **fit_kwargs)\u001b[0m\n\u001b[0;32m   1924\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m training_log_writer(log_file_name, append_log) \u001b[38;5;28;01mas\u001b[39;00m save_helper:\n\u001b[0;32m   1925\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_log \u001b[38;5;241m=\u001b[39m save_helper\n\u001b[1;32m-> 1926\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\flaml\\automl\\automl.py:2485\u001b[0m, in \u001b[0;36mAutoML._search\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_sequential()\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2486\u001b[0m \u001b[38;5;66;03m# Add a checkpoint for the current best config to the log.\u001b[39;00m\n\u001b[0;32m   2487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_log:\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\flaml\\automl\\automl.py:2058\u001b[0m, in \u001b[0;36mAutoML._search_parallel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2054\u001b[0m resources_per_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mresources_per_trial\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_spark:\n\u001b[0;32m   2057\u001b[0m     \u001b[38;5;66;03m# use spark as parallel backend\u001b[39;00m\n\u001b[1;32m-> 2058\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2060\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_alg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_alg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_budget_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_budget_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_spark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_cancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_cancel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# raise_on_failed_trial=False,\u001b[39;49;00m\n\u001b[0;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# keep_checkpoints_num=1,\u001b[39;49;00m\n\u001b[0;32m   2072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# checkpoint_score_attr=\"min-val_loss\",\u001b[39;49;00m\n\u001b[0;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# use ray as parallel backend\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mtune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   2077\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable,\n\u001b[0;32m   2078\u001b[0m         search_alg\u001b[38;5;241m=\u001b[39msearch_alg,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2089\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_ray \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_ray, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[0;32m   2090\u001b[0m     )\n",
      "File \u001b[1;32md:\\Downloads\\Documents\\vscode\\.venv\\Lib\\site-packages\\flaml\\tune\\tune.py:662\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(evaluation_function, config, low_cost_partial_config, cat_hp_cost, metric, mode, time_budget_s, points_to_evaluate, evaluated_rewards, resource_attr, min_resource, max_resource, reduction_factor, scheduler, search_alg, verbose, local_dir, num_samples, resources_per_trial, config_constraints, metric_constraints, max_failure, use_ray, use_spark, use_incumbent_result_in_evaluation, log_file_name, lexico_objectives, force_cancel, n_concurrent_trials, **ray_args)\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Try pip install flaml[spark] or set use_spark=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflaml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearcher\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msuggestion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConcurrencyLimiter\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrial_runner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkTrialRunner\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'joblibspark'. Try pip install flaml[spark] or set use_spark=False."
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "automl = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 30,  # total running time in seconds\n",
    "    \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'regression',  # task type    \n",
    "    \"log_file_name\": 'houses_experiment.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "    \"use_spark\": False,  # whether to use Spark for distributed training\n",
    "    \"n_concurrent_trials\": 2,  # the maximum number of concurrent trials\n",
    "}\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,independent_demographic+independent_investigations],\n",
    "                                                    df.loc[:,dependent_variable],\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=df.loc[:,dependent_variable],\n",
    "                                                    random_state=11)\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
    "automl.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.barh(automl.feature_names_in_, automl.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = automl.predict(X_test)\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))\n",
    "print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))\n",
    "print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "    }\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    callbacks=[early_stopping(100), log_evaluation(100)],\n",
    ")\n",
    "\n",
    "prediction = np.rint(model.predict(X_test, num_iteration=model.best_iteration))\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "\n",
    "best_params = model.params\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"  Accuracy = {}\".format(accuracy))\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    dval = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        valid_sets=[dtrain, dval],\n",
    "        callbacks=[early_stopping(100), log_evaluation(100)],\n",
    "    )\n",
    "\n",
    "    prediction = np.rint(model.predict(val_x, num_iteration=model.best_iteration))\n",
    "    accuracy = accuracy_score(val_y, prediction)\n",
    "\n",
    "    best_params = model.params\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(\"  Accuracy = {}\".format(accuracy))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the gendfer\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "# \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "\n",
    "# Check for duplication\n",
    "duplicated_df, to_drop_duplicated_df = mphd.pre_processing.check_duplication(df)\n",
    "\n",
    "# Check for missing value\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To fix AGR == ' ' issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Globulin for every patient ID with globulin = tp - alb based on resource below:\n",
    "# https://www.ncbi.nlm.nih.gov/books/NBK204/#:~:text=The%20total%20globulin%20fraction%20is,of%20further%20fractionating%20serum%20proteins\n",
    "\n",
    "# To check the truthness of this on the data\n",
    "# Create a deep copy of the df with AGR not null first\n",
    "temp_df = df.query(\"AGR.notnull()\").copy(deep = True)\n",
    "\n",
    "# Calculatet the globulin and agr_ratio\n",
    "def calculate_agr(df:pd.DataFrame, column_name:str):\n",
    "    df.loc[:,column_name] = df.loc[:,\"ALB\"] / (df.loc[:,\"TP\"] - df.loc[:,\"ALB\"])\n",
    "    return df\n",
    "\n",
    "# Calculate the approximate agr\n",
    "temp_df = calculate_agr(df = temp_df, column_name = \"agr_new\")\n",
    "# Check for float similarity\n",
    "temp_df.loc[:,\"agr_similarity\"] = temp_df.loc[:,(\"AGR\", \"agr_new\",)].apply(lambda x: np.isclose(float(x[0]), x[1], rtol = 0.1), axis = 1)\n",
    "\n",
    "# Pivot the information\n",
    "pt = temp_df.pivot_table(index = \"agr_similarity\", values = \"Patient_ID\", aggfunc = len, margins = True).rename(columns={\"Patient_ID\":\"count\"})\n",
    "# Calculate percentage\n",
    "pt.loc[:,\"percentage\"] = round(pt.loc[:,\"count\"] / pt.loc[\"All\", \"count\"] * 100, 2)\n",
    "\n",
    "print(pt.to_markdown(tablefmt = \"pretty\"))\n",
    "\n",
    "# Check on the not similarity result\n",
    "temp_df.query(\"agr_similarity == False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial to impute with calculation\n",
    "missing_df = calculate_agr(df = missing_df, column_name=\"AGR\")\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with MICE\n",
    "# https://medium.com/@brijesh_soni/topic-9-mice-or-multivariate-imputation-with-chain-equation-f8fd435ca91#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,produce%20a%20final%20imputed%20dataset.\n",
    "\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")\n",
    "# Check on the imputated value\n",
    "df.loc[missing_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__intepretation__:\n",
    "\n",
    "For imputation, despite the logic of how AGR being calculated, there is a lot of noise in the data for AGR value, therefore we would use MICE for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Encode\n",
    "data = mphd.categorical_data.reverse_encode(df = df, json_dict=data_dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normal distribution\n",
    "normal_distribution_list, abnormal_distribution_list = mphd.continous_data.descriptive_analysis(df = df, \n",
    "                                                                                                independent_variables=independent_continous, \n",
    "                                                                                                dependent_variables = dependent_variable,\n",
    "                                                                                                descriptive_type = \"continous\",\n",
    "                                                                                                plot_dependent_variables = False,\n",
    "                                                                                                plot_correlation = False, \n",
    "                                                                                                round = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summary = mphd.categorical_data.categorical_descriptive_analysis(data,\n",
    "                                                                             independent_variables = independent_categorical, \n",
    "                                                                             dependent_variables = dependent_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# set acceptable p value\n",
    "acceptable_p_value = 0.05\n",
    "\n",
    "# For binominal logistic regression with 2 different depression score outcome along with all independent variable are categorized\n",
    "logistic_models, summary_logistic_models = mphd.regression.regression_list(df = df, mode = \"sm.Logit\",\n",
    "                                                                                   independent_variables = independent_demographic + independent_investigations,\n",
    "                                                                                   dependent_variables = dependent_variable,\n",
    "                                                                                   p_value_cut_off = acceptable_p_value)\n",
    "\n",
    "# To display some information\n",
    "columns_to_display = (\"pseudo_r_2\", \"log_likelihood\", \"llr_p_value\", \"aic_akaike_information_criterion\", \"bic_bayesin_information_criterion\", \"coeff_all_significant\")\n",
    "summary_logistic_short = mphd.regression.analyse_model_summary(summary_logistic_models.loc[:,(\"variables\", \"num_variables\") + columns_to_display + \n",
    "                                                                                      (\"roc\", \"shapiro_residual\", \n",
    "                                                                                       \"Lagrange_Multiplier\", \"Lagrange_Multiplier_p-value\",\n",
    "                                                                                       \"F-statistic\", \"F-statistic_p-value\")], \n",
    "                                                      top_count = 3,\n",
    "                                                      parameters= {\"aic_akaike_information_criterion\": True,\n",
    "                                                                   \"bic_bayesin_information_criterion\": True,\n",
    "                                                                   \"pseudo_r_2\": False, \n",
    "                                                                   \"roc\":False})\n",
    "print(summary_logistic_short.round(4).to_markdown(tablefmt = \"pretty\"))\n",
    "print(logistic_models[2893].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "num_cols = ['age', 'bmi', 'sys_bp', 'dias_bp', 'hba1c', 'ldl']\n",
    "cat_cols = ['sex', 'ethnic', 'retinopathy', 'ihd', 'cevd', 'nephropathy']\n",
    "\n",
    "n_estimators = [100, 300]\n",
    "max_depth = [3,4,5]\n",
    "min_child_weight = range(1,3,1)\n",
    "booster = ['gbdt']\n",
    "base_score = [0.5,0.6]\n",
    "learning_rate = [0.1,0.2]\n",
    "objective = ['binary']\n",
    "seed = [27]\n",
    "gamma= [0.7,0.8,0.9]\n",
    "colsample_bytree=[0.7,0.8,0.9]\n",
    "subsample=[0.6,0.7,0.8]\n",
    "reg_alpha = [1e-5,0.01,0.03]\n",
    "weights = np.linspace(0.3, 0.9, 2)\n",
    "num_leaves = [6]\n",
    "\n",
    "lgbm_params = {'classifier__n_estimators': n_estimators, 'classifier__max_depth': max_depth,\n",
    "               'classifier__learning_rate' : learning_rate, 'classifier__min_child_weight' : min_child_weight, \n",
    "               'classifier__boosting_type' : booster, 'classifier__seed':seed,'smote__sampling_strategy': weights,\n",
    "               'classifier__reg_alpha':reg_alpha, 'classifier__num_leaves':num_leaves}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,independent_demographic+independent_investigations],\n",
    "                                                    df.loc[:,dependent_variable],\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=df.loc[:,dependent_variable],\n",
    "                                                    random_state=11)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), independent_continous),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([('smote', SMOTE(random_state=11)),\n",
    "                     ('scaler', preprocessor),\n",
    "                     ('classifier', LGBMClassifier())])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=11)\n",
    "    \n",
    "param_grid = lgbm_params\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=stratified_kfold,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "model = grid_search.fit(X_train, y_train, num_boost_round=1000, early_stopping_rounds=50, verbose_eval=100)\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
