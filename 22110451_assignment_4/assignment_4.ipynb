{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip FLAML \"flaml[spark]\" setuptools wheel optuna optuna-integration xgboost catboost imbalanced-learn\n",
    "# %pip install --upgrade \"scikit-learn==1.1.2\"\n",
    "# %pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow own package to be imported\n",
    "import sys\n",
    "import os\n",
    "if os.path.dirname(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action = \"ignore\")\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import wh0102 as mphd\n",
    "\n",
    "# Prepare the data dictionary\n",
    "data_dictionary = {\n",
    "    \"Ethnic\":{0:\"Malay\", 1:\"Chinese\", 2:\"Indian\"},\n",
    "    \"bmi\":{0:\"Normal BMI\", 1:\"Overweight\"},\n",
    "    \"Disease\":{0:\"No liver disease\", 1:\"Have Liver Disease\"},\n",
    "    \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "}\n",
    "\n",
    "# Rename for easier references\n",
    "column_to_be_rename = {\"Sgot\":\"ALT\",\n",
    "                       \"Sgpt\":\"AST\",\n",
    "                       \"Alkphos\":\"ALP\"}\n",
    "\n",
    "# Prepare the variables\n",
    "dependent_variable = \"Disease\"\n",
    "independent_demographic = (\"Age\", \"Gender\", \"Ethnic\", \"bmi\",)\n",
    "independent_investigations = (\"AGR\", \"ALB\", \"TP\", \"TB\", \"DB\", \"ALP\", \"ALT\", \"AST\",)\n",
    "independent_continous = (independent_demographic[0],) + independent_investigations\n",
    "independent_categorical = independent_demographic[1:]\n",
    "independent_variables = independent_demographic + independent_investigations\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r\"assignment4.csv\")\n",
    "\n",
    "# Rename the column name\n",
    "df = df.rename(columns = column_to_be_rename)\n",
    "\n",
    "# To reassign the categorical value\n",
    "for column in [key for key in data_dictionary.keys() if key != \"Gender\"]:\n",
    "    df.loc[:,column] = df.loc[:,column] - 1\n",
    "\n",
    "# Print the information\n",
    "df.info()\n",
    "\n",
    "# To delete after this\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")\n",
    "# df.drop(columns = [\"Patient_ID\"]).to_csv(r\"imputed_assignment_4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = mphd.pre_processing.train_test_split(df = df,\n",
    "                                                                        independent_variables=independent_variables,\n",
    "                                                                        dependent_variable = dependent_variable,\n",
    "                                                                        test_size = 0.2)\n",
    "# \"estimator_list\": ['lgbm', 'lgbm_spark', 'xgboost', 'catboost'],\n",
    "settings = {\n",
    "    \"time_budget\": 120,  # total running time in seconds\n",
    "    \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['xgboost',],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'classification',  # task type    \n",
    "    \"log_file_name\": None,  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "    \"use_spark\": True,  # whether to use Spark for distributed training\n",
    "    \"n_concurrent_trials\": 2,  # the maximum number of concurrent trials\n",
    "}\n",
    "\n",
    "automl = mphd.machine_learning.automl(X_train=X_train, y_train=y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mphd.analyse_ml.analyse_automl(automl=automl, X_test = X_test, y_test = y_test)\n",
    "automl.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna.integration.lightgbm as lgb\n",
    "from optuna.integration import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "    }\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    callbacks=[early_stopping(100), log_evaluation(100)],\n",
    ")\n",
    "\n",
    "prediction = np.rint(model.predict(X_test, num_iteration=model.best_iteration))\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "\n",
    "best_params = model.params\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"  Accuracy = {}\".format(accuracy))\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the gender\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "# \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "\n",
    "# Check for duplication\n",
    "duplicated_df, to_drop_duplicated_df = mphd.pre_processing.check_duplication(df)\n",
    "\n",
    "# Check for missing value\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To fix AGR == ' ' issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Globulin for every patient ID with globulin = tp - alb based on resource below:\n",
    "# https://www.ncbi.nlm.nih.gov/books/NBK204/#:~:text=The%20total%20globulin%20fraction%20is,of%20further%20fractionating%20serum%20proteins\n",
    "\n",
    "# To check the truthness of this on the data\n",
    "# Create a deep copy of the df with AGR not null first\n",
    "temp_df = df.query(\"AGR.notnull()\").copy(deep = True)\n",
    "\n",
    "# Calculatet the globulin and agr_ratio\n",
    "def calculate_agr(df:pd.DataFrame, column_name:str):\n",
    "    df.loc[:,column_name] = df.loc[:,\"ALB\"] / (df.loc[:,\"TP\"] - df.loc[:,\"ALB\"])\n",
    "    return df\n",
    "\n",
    "# Calculate the approximate agr\n",
    "temp_df = calculate_agr(df = temp_df, column_name = \"agr_new\")\n",
    "# Check for float similarity\n",
    "temp_df.loc[:,\"agr_similarity\"] = temp_df.loc[:,(\"AGR\", \"agr_new\",)].apply(lambda x: np.isclose(float(x[0]), x[1], rtol = 0.1), axis = 1)\n",
    "\n",
    "# Pivot the information\n",
    "pt = temp_df.pivot_table(index = \"agr_similarity\", values = \"Patient_ID\", aggfunc = len, margins = True).rename(columns={\"Patient_ID\":\"count\"})\n",
    "# Calculate percentage\n",
    "pt.loc[:,\"percentage\"] = round(pt.loc[:,\"count\"] / pt.loc[\"All\", \"count\"] * 100, 2)\n",
    "\n",
    "print(pt.to_markdown(tablefmt = \"pretty\"))\n",
    "\n",
    "# Check on the not similarity result\n",
    "temp_df.query(\"agr_similarity == False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial to impute with calculation\n",
    "missing_df = calculate_agr(df = missing_df, column_name=\"AGR\")\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with MICE\n",
    "# https://medium.com/@brijesh_soni/topic-9-mice-or-multivariate-imputation-with-chain-equation-f8fd435ca91#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,produce%20a%20final%20imputed%20dataset.\n",
    "\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")\n",
    "# Check on the imputated value\n",
    "df.loc[missing_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__intepretation__:\n",
    "\n",
    "For imputation, despite the logic of how AGR being calculated, there is a lot of noise in the data for AGR value, therefore we would use MICE for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Encode\n",
    "data = mphd.categorical_data.reverse_encode(df = df, json_dict=data_dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normal distribution\n",
    "normal_distribution_list, abnormal_distribution_list = mphd.continous_data.descriptive_analysis(df = df, \n",
    "                                                                                                independent_variables=independent_continous, \n",
    "                                                                                                dependent_variables = dependent_variable,\n",
    "                                                                                                descriptive_type = \"continous\",\n",
    "                                                                                                plot_dependent_variables = False,\n",
    "                                                                                                plot_correlation = False, \n",
    "                                                                                                round = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summary = mphd.categorical_data.categorical_descriptive_analysis(data,\n",
    "                                                                             independent_variables = independent_categorical, \n",
    "                                                                             dependent_variables = dependent_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# set acceptable p value\n",
    "acceptable_p_value = 0.05\n",
    "\n",
    "# For binominal logistic regression with 2 different depression score outcome along with all independent variable are categorized\n",
    "logistic_models, summary_logistic_models = mphd.regression.regression_list(df = df, mode = \"sm.Logit\",\n",
    "                                                                                   independent_variables = independent_demographic + independent_investigations,\n",
    "                                                                                   dependent_variables = dependent_variable,\n",
    "                                                                                   p_value_cut_off = acceptable_p_value)\n",
    "\n",
    "# To display some information\n",
    "columns_to_display = (\"pseudo_r_2\", \"log_likelihood\", \"llr_p_value\", \"aic_akaike_information_criterion\", \"bic_bayesin_information_criterion\", \"coeff_all_significant\")\n",
    "summary_logistic_short = mphd.regression.analyse_model_summary(summary_logistic_models.loc[:,(\"variables\", \"num_variables\") + columns_to_display + \n",
    "                                                                                      (\"roc\", \"shapiro_residual\", \n",
    "                                                                                       \"Lagrange_Multiplier\", \"Lagrange_Multiplier_p-value\",\n",
    "                                                                                       \"F-statistic\", \"F-statistic_p-value\")], \n",
    "                                                      top_count = 3,\n",
    "                                                      parameters= {\"aic_akaike_information_criterion\": True,\n",
    "                                                                   \"bic_bayesin_information_criterion\": True,\n",
    "                                                                   \"pseudo_r_2\": False, \n",
    "                                                                   \"roc\":False})\n",
    "print(summary_logistic_short.round(4).to_markdown(tablefmt = \"pretty\"))\n",
    "print(logistic_models[2893].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "num_cols = ['age', 'bmi', 'sys_bp', 'dias_bp', 'hba1c', 'ldl']\n",
    "cat_cols = ['sex', 'ethnic', 'retinopathy', 'ihd', 'cevd', 'nephropathy']\n",
    "\n",
    "n_estimators = [100, 300]\n",
    "max_depth = [3,4,5]\n",
    "min_child_weight = range(1,3,1)\n",
    "booster = ['gbdt']\n",
    "base_score = [0.5,0.6]\n",
    "learning_rate = [0.1,0.2]\n",
    "objective = ['binary']\n",
    "seed = [27]\n",
    "gamma= [0.7,0.8,0.9]\n",
    "colsample_bytree=[0.7,0.8,0.9]\n",
    "subsample=[0.6,0.7,0.8]\n",
    "reg_alpha = [1e-5,0.01,0.03]\n",
    "weights = np.linspace(0.3, 0.9, 2)\n",
    "num_leaves = [6]\n",
    "\n",
    "lgbm_params = {'classifier__n_estimators': n_estimators, 'classifier__max_depth': max_depth,\n",
    "               'classifier__learning_rate' : learning_rate, 'classifier__min_child_weight' : min_child_weight, \n",
    "               'classifier__boosting_type' : booster, 'classifier__seed':seed,'smote__sampling_strategy': weights,\n",
    "               'classifier__reg_alpha':reg_alpha, 'classifier__num_leaves':num_leaves}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,independent_demographic+independent_investigations],\n",
    "                                                    df.loc[:,dependent_variable],\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=df.loc[:,dependent_variable],\n",
    "                                                    random_state=11)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), independent_continous),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([('smote', SMOTE(random_state=11)),\n",
    "                     ('scaler', preprocessor),\n",
    "                     ('classifier', LGBMClassifier())])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=11)\n",
    "    \n",
    "param_grid = lgbm_params\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=stratified_kfold,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "model = grid_search.fit(X_train, y_train, num_boost_round=1000, early_stopping_rounds=50, verbose_eval=100)\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
