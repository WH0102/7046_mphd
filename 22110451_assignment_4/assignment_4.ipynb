{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allow own package to be imported\n",
    "import sys\n",
    "import os\n",
    "if os.path.dirname(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action = \"ignore\")\n",
    "\n",
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import wh0102 as mphd\n",
    "\n",
    "# Prepare the data dictionary\n",
    "data_dictionary = {\n",
    "    \"Ethnic\":{0:\"Malay\", 1:\"Chinese\", 2:\"Indian\"},\n",
    "    \"bmi\":{0:\"Normal BMI\", 1:\"Overweight\"},\n",
    "    \"Disease\":{0:\"No liver disease\", 1:\"Have Liver Disease\"},\n",
    "    \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "}\n",
    "\n",
    "# Prepare the variables\n",
    "dependent_variable = \"Disease\"\n",
    "independent_demographic = (\"Age\", \"Gender\", \"Ethnic\", \"bmi\",)\n",
    "independent_investigations = (\"AGR\", \"ALB\", \"TP\", \"TB\", \"DB\", \"Alkphos\", \"Sgot\", \"Sgpt\",)\n",
    "independent_continous = (independent_demographic[0],) + independent_investigations\n",
    "independent_categorical = independent_demographic[1:]\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r\"MQB7046 Assignment 4.csv\")\n",
    "\n",
    "# To reassign the categorical value\n",
    "for column in [key for key in data_dictionary.keys() if key != \"Gender\"]:\n",
    "    df.loc[:,column] = df.loc[:,column] - 1\n",
    "\n",
    "# Print the information\n",
    "# df.info()\n",
    "\n",
    "# To delete after this\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "automl = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 30,  # total running time in seconds\n",
    "    \"metric\": 'r2',  # primary metrics for regression can be chosen from: ['mae','mse','r2','rmse','mape']\n",
    "    \"estimator_list\": ['lgbm'],  # list of ML learners; we tune lightgbm in this example\n",
    "    \"task\": 'regression',  # task type    \n",
    "    \"log_file_name\": 'houses_experiment.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "    \"use_spark\": True,  # whether to use Spark for distributed training\n",
    "    \"n_concurrent_trials\": 2,  # the maximum number of concurrent trials\n",
    "}\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,independent_demographic+independent_investigations],\n",
    "                                                    df.loc[:,dependent_variable],\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=df.loc[:,dependent_variable],\n",
    "                                                    random_state=11)\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparmeter config:', automl.best_config)\n",
    "print('Best r2 on validation data: {0:.4g}'.format(1-automl.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl.best_config_train_time))\n",
    "automl.model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.barh(automl.feature_names_in_, automl.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = automl.predict(X_test)\n",
    "from flaml.ml import sklearn_metric_loss_score\n",
    "print('r2', '=', 1 - sklearn_metric_loss_score('r2', y_pred, y_test))\n",
    "print('mse', '=', sklearn_metric_loss_score('mse', y_pred, y_test))\n",
    "print('mae', '=', sklearn_metric_loss_score('mae', y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "    }\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    callbacks=[early_stopping(100), log_evaluation(100)],\n",
    ")\n",
    "\n",
    "prediction = np.rint(model.predict(X_test, num_iteration=model.best_iteration))\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "\n",
    "best_params = model.params\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"  Accuracy = {}\".format(accuracy))\n",
    "print(\"  Params: \")\n",
    "for key, value in best_params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    dval = lgb.Dataset(val_x, label=val_y)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        valid_sets=[dtrain, dval],\n",
    "        callbacks=[early_stopping(100), log_evaluation(100)],\n",
    "    )\n",
    "\n",
    "    prediction = np.rint(model.predict(val_x, num_iteration=model.best_iteration))\n",
    "    accuracy = accuracy_score(val_y, prediction)\n",
    "\n",
    "    best_params = model.params\n",
    "    print(\"Best params:\", best_params)\n",
    "    print(\"  Accuracy = {}\".format(accuracy))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the gendfer\n",
    "df = mphd.categorical_data.label_encode(df = df, columns = \"Gender\", convert_numeric=True)\n",
    "# \"Gender\":{0:\"Female\", 1:\"Male\"}\n",
    "\n",
    "# Check for duplication\n",
    "duplicated_df, to_drop_duplicated_df = mphd.pre_processing.check_duplication(df)\n",
    "\n",
    "# Check for missing value\n",
    "missing_df = mphd.missing_values.analyse_missing_row(df)\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To fix AGR == ' ' issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Globulin for every patient ID with globulin = tp - alb based on resource below:\n",
    "# https://www.ncbi.nlm.nih.gov/books/NBK204/#:~:text=The%20total%20globulin%20fraction%20is,of%20further%20fractionating%20serum%20proteins\n",
    "\n",
    "# To check the truthness of this on the data\n",
    "# Create a deep copy of the df with AGR not null first\n",
    "temp_df = df.query(\"AGR.notnull()\").copy(deep = True)\n",
    "\n",
    "# Calculatet the globulin and agr_ratio\n",
    "def calculate_agr(df:pd.DataFrame, column_name:str):\n",
    "    df.loc[:,column_name] = df.loc[:,\"ALB\"] / (df.loc[:,\"TP\"] - df.loc[:,\"ALB\"])\n",
    "    return df\n",
    "\n",
    "# Calculate the approximate agr\n",
    "temp_df = calculate_agr(df = temp_df, column_name = \"agr_new\")\n",
    "# Check for float similarity\n",
    "temp_df.loc[:,\"agr_similarity\"] = temp_df.loc[:,(\"AGR\", \"agr_new\",)].apply(lambda x: np.isclose(float(x[0]), x[1], rtol = 0.1), axis = 1)\n",
    "\n",
    "# Pivot the information\n",
    "pt = temp_df.pivot_table(index = \"agr_similarity\", values = \"Patient_ID\", aggfunc = len, margins = True).rename(columns={\"Patient_ID\":\"count\"})\n",
    "# Calculate percentage\n",
    "pt.loc[:,\"percentage\"] = round(pt.loc[:,\"count\"] / pt.loc[\"All\", \"count\"] * 100, 2)\n",
    "\n",
    "print(pt.to_markdown(tablefmt = \"pretty\"))\n",
    "\n",
    "# Check on the not similarity result\n",
    "temp_df.query(\"agr_similarity == False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial to impute with calculation\n",
    "missing_df = calculate_agr(df = missing_df, column_name=\"AGR\")\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with MICE\n",
    "# https://medium.com/@brijesh_soni/topic-9-mice-or-multivariate-imputation-with-chain-equation-f8fd435ca91#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,produce%20a%20final%20imputed%20dataset.\n",
    "\n",
    "df = mphd.missing_values.mice_imputation(df = df, columns = \"AGR\")\n",
    "# Check on the imputated value\n",
    "df.loc[missing_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__intepretation__:\n",
    "\n",
    "For imputation, despite the logic of how AGR being calculated, there is a lot of noise in the data for AGR value, therefore we would use MICE for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Encode\n",
    "data = mphd.categorical_data.reverse_encode(df = df, json_dict=data_dictionary)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normal distribution\n",
    "normal_distribution_list, abnormal_distribution_list = mphd.continous_data.descriptive_analysis(df = df, \n",
    "                                                                                                independent_variables=independent_continous, \n",
    "                                                                                                dependent_variables = dependent_variable,\n",
    "                                                                                                descriptive_type = \"continous\",\n",
    "                                                                                                plot_dependent_variables = False,\n",
    "                                                                                                plot_correlation = False, \n",
    "                                                                                                round = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_summary = mphd.categorical_data.categorical_descriptive_analysis(data,\n",
    "                                                                             independent_variables = independent_categorical, \n",
    "                                                                             dependent_variables = dependent_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# set acceptable p value\n",
    "acceptable_p_value = 0.05\n",
    "\n",
    "# For binominal logistic regression with 2 different depression score outcome along with all independent variable are categorized\n",
    "logistic_models, summary_logistic_models = mphd.regression.regression_list(df = df, mode = \"sm.Logit\",\n",
    "                                                                                   independent_variables = independent_demographic + independent_investigations,\n",
    "                                                                                   dependent_variables = dependent_variable,\n",
    "                                                                                   p_value_cut_off = acceptable_p_value)\n",
    "\n",
    "# To display some information\n",
    "columns_to_display = (\"pseudo_r_2\", \"log_likelihood\", \"llr_p_value\", \"aic_akaike_information_criterion\", \"bic_bayesin_information_criterion\", \"coeff_all_significant\")\n",
    "summary_logistic_short = mphd.regression.analyse_model_summary(summary_logistic_models.loc[:,(\"variables\", \"num_variables\") + columns_to_display + \n",
    "                                                                                      (\"roc\", \"shapiro_residual\", \n",
    "                                                                                       \"Lagrange_Multiplier\", \"Lagrange_Multiplier_p-value\",\n",
    "                                                                                       \"F-statistic\", \"F-statistic_p-value\")], \n",
    "                                                      top_count = 3,\n",
    "                                                      parameters= {\"aic_akaike_information_criterion\": True,\n",
    "                                                                   \"bic_bayesin_information_criterion\": True,\n",
    "                                                                   \"pseudo_r_2\": False, \n",
    "                                                                   \"roc\":False})\n",
    "print(summary_logistic_short.round(4).to_markdown(tablefmt = \"pretty\"))\n",
    "print(logistic_models[2893].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "num_cols = ['age', 'bmi', 'sys_bp', 'dias_bp', 'hba1c', 'ldl']\n",
    "cat_cols = ['sex', 'ethnic', 'retinopathy', 'ihd', 'cevd', 'nephropathy']\n",
    "\n",
    "n_estimators = [100, 300]\n",
    "max_depth = [3,4,5]\n",
    "min_child_weight = range(1,3,1)\n",
    "booster = ['gbdt']\n",
    "base_score = [0.5,0.6]\n",
    "learning_rate = [0.1,0.2]\n",
    "objective = ['binary']\n",
    "seed = [27]\n",
    "gamma= [0.7,0.8,0.9]\n",
    "colsample_bytree=[0.7,0.8,0.9]\n",
    "subsample=[0.6,0.7,0.8]\n",
    "reg_alpha = [1e-5,0.01,0.03]\n",
    "weights = np.linspace(0.3, 0.9, 2)\n",
    "num_leaves = [6]\n",
    "\n",
    "lgbm_params = {'classifier__n_estimators': n_estimators, 'classifier__max_depth': max_depth,\n",
    "               'classifier__learning_rate' : learning_rate, 'classifier__min_child_weight' : min_child_weight, \n",
    "               'classifier__boosting_type' : booster, 'classifier__seed':seed,'smote__sampling_strategy': weights,\n",
    "               'classifier__reg_alpha':reg_alpha, 'classifier__num_leaves':num_leaves}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,independent_demographic+independent_investigations],\n",
    "                                                    df.loc[:,dependent_variable],\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=df.loc[:,dependent_variable],\n",
    "                                                    random_state=11)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), independent_continous),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([('smote', SMOTE(random_state=11)),\n",
    "                     ('scaler', preprocessor),\n",
    "                     ('classifier', LGBMClassifier())])\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=11)\n",
    "    \n",
    "param_grid = lgbm_params\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=stratified_kfold,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "model = grid_search.fit(X_train, y_train, num_boost_round=1000, early_stopping_rounds=50, verbose_eval=100)\n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
